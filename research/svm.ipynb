{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "svm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thefr33radical/projects/blob/master/research/svm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "kV6jwOqPiOnd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machine\n",
        "\n",
        "#### Implementation of SVM from scratch\n",
        "\n",
        "##### References\n",
        "* https://machinelearningmastery.com/machine-learning-in-python-step-by-step/\n",
        "* http://machinelearningmastery.com/support-vector-machines-for-machine-learning/\n",
        "*  http://scikit-learn.org/stable/modules/svm.html\n",
        "* https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47"
      ]
    },
    {
      "metadata": {
        "id": "nYscIth1iO68",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "  \"\"\"\n",
        "  Function to read csv data from path/url\n",
        "  \"\"\"\n",
        "  \n",
        "  data = { -1: np.array([[1,7],[2,8],[3,8]]), 1:np.array([[5,1],[6,-1],[7,3]])}\n",
        "  \n",
        "  return data\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "udYJjpWZ84yK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install seaborn\n",
        "#!pip install matplotlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "def load_data(path):\n",
        "  \"\"\"\n",
        "  Function to read csv data from path/url\n",
        "  \"\"\"\n",
        "  names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
        "  dataset = pd.read_csv(path, names= names)\n",
        "  \n",
        "  # Select only two classes for binary classification\n",
        "  dataset1 = dataset[dataset[\"class\"] == \"Iris-setosa\"]\n",
        "  dataset2 = dataset[dataset[\"class\"] == \"Iris-versicolor\"]\n",
        "  dataset = pd.concat([dataset1,dataset2])  \n",
        "  dataset[\"class\"]= LabelEncoder().fit_transform(dataset[\"class\"])\n",
        "  return dataset\n",
        "\n",
        "def preprocess(dataset):\n",
        "  \"\"\"\n",
        "  Function to scale data between 0 & 1\n",
        "  \"\"\"  \n",
        "  dataset.iloc[:,:-1] = MinMaxScaler().fit_transform(dataset.iloc[:,:-1])\n",
        "  return dataset\n",
        "\n",
        "def analyse(dataset):\n",
        "  \"\"\"\n",
        "  Function to data distributions and plot the graphs\n",
        "  \"\"\"\n",
        "  print(dataset.describe())\n",
        "  plot_var = dataset\n",
        "  sns.pairplot(plot_var,hue=\"class\",markers=\"*\")\n",
        "  plt.show()\n",
        "  \n",
        "def predict(weights, feature):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    return weights.T,feature\n",
        "   \n",
        "  \n",
        "def gradient_descent(weights,feature):\n",
        "  \"\"\"\n",
        "  \n",
        "  \"\"\"\n",
        "  predict_y =[]\n",
        "  for i in range(len(feature)):   \n",
        "    predict_y.append(predict(weights, feature[i,:]))\n",
        "    #print(\"predicted\", predict_y)\n",
        "  product = np.dot(y,predict_y)\n",
        "    #print(\"product\", product)\n",
        "  for i in range(len(weights)):\n",
        "      if(product>=1):\n",
        "        cost = 0\n",
        "        wts=   2 * alpha * weights[i]\n",
        "        weights-=wts*lr\n",
        "\n",
        "      else:\n",
        "        cost = 1-product\n",
        "        wts = ( 2 * alpha * weights[i])- product \n",
        "        weights[i]-=wts*lr\n",
        "      epochs+=1\n",
        "  print(\"weights\", weights)\n",
        "  model = SVC(kernel=\"linear\")\n",
        "  model.fit(feature,y)\n",
        "  print(model.coef_)\n",
        "  \n",
        "def train(dataset):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  weights = np.array([random.randint(1,2)]*(len(dataset.columns)-1),dtype=float)\n",
        "  feature = dataset.iloc[:,:-1].copy()\n",
        "  print(weights)\n",
        "  y = dataset.iloc[:,-1]\n",
        "  epochs = 1\n",
        "  lr = 0.0001\n",
        "  \n",
        "  while epochs< 100:\n",
        "    \n",
        "    alpha = 1/epochs\n",
        "    weights,bias = gradient_descent(weights,feature)\n",
        "    \n",
        "     \n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  dataset = load_data(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\")\n",
        "  dataset = preprocess(dataset)\n",
        "  #analyse(dataset)\n",
        "  train_input,train_output,test_input,test_output = train_test_split(dataset.iloc[:,:-1],dataset.iloc[:,-1], test_size =0.1)\n",
        "  train(dataset)\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}