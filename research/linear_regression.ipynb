{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear_regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "q95zjTkespJg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k_6j1aM3stas",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Implementation  of Linear Regression using Least Squares method.  Optimization function is Gradient Descent.\n"
      ]
    },
    {
      "metadata": {
        "id": "HFQ2upKuxF9G",
        "colab_type": "code",
        "outputId": "4de234c2-cdfb-4463-c175-e18219c8a1a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3893
        }
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "count = 0\n",
        "\n",
        "def get_dataset():\n",
        "  \"\"\"\n",
        "  Define/ load dataset here\n",
        "  \"\"\"  \n",
        "  dataset = load_boston()\n",
        "  boston_data = pd.DataFrame(dataset.data, columns = dataset.feature_names)\n",
        "  boston_data[\"y\"] = dataset.target\n",
        "  return boston_data.iloc[:100,:]\n",
        "\n",
        "def least_squares(dataset,weights,bias):\n",
        "  \"\"\"\n",
        "  Function to compute least squares\n",
        "  dataset: Pandas dataframe\n",
        "  \n",
        "  \"\"\"  \n",
        "  yp =[ ]\n",
        "  mse = 0.0\n",
        "  for i in range(len(dataset)):\n",
        "    x_variables = np.array(dataset.iloc[i,:-1]).copy()\n",
        "    temp = predict_function(weights,x_variables,bias)\n",
        "    yp.append(temp)\n",
        "    diff =dataset.iloc[i,-1]- temp \n",
        "    print(\"diff\",diff)\n",
        "    mse += diff*diff    \n",
        " \n",
        "  \"\"\"\n",
        "  save prediction for each iteration\n",
        "  count+=1\n",
        "  dataset[\"prediction\"+str(count)]=yp\n",
        "  \"\"\"  \n",
        "  dataset[\"predicted\"]=yp\n",
        "    \n",
        "  print(mse/len(dataset))\n",
        "  return mse/len(dataset)\n",
        "  \n",
        "def predict_function(weights,x_variables,bias):\n",
        "  \"\"\"\n",
        "  prediction function\n",
        "  weights: numpy array\n",
        "  x_variables: numpy array\n",
        "  bias: float\n",
        "  \n",
        "  return float\n",
        "  \"\"\"\n",
        "  y_predicted = np.dot(weights.T,x_variables)+ bias\n",
        "  #print((weights.T,x_variables))\n",
        "  #print(\"y predicted\",y_predicted)\n",
        "  return y_predicted\n",
        "\n",
        "def gradient_descent(data,weights,bias):\n",
        "  \"\"\"\n",
        "  \n",
        "  \"\"\"\n",
        "  weights_new = np.array([0]*len(weights))\n",
        "  bias_new=0.0\n",
        "  learning_rate=0.001\n",
        "  yp =[]\n",
        "  \n",
        "  for i in range(len(data)):\n",
        "    yp.append(predict_function(weights,x_variables,bias))\n",
        "    \n",
        "  yp = np.array(yp)\n",
        "  yp =yp.T\n",
        "  data[\"predicted\"] = yp\n",
        "  \n",
        "  for row in range(len(data)):\n",
        "    for col in range(data.columns-2):\n",
        "      weights[j]+= -2 * ( data.iloc[i,j]*(data[i,-1]-data.iloc[i-2]) )\n",
        "    \n",
        "    bias_new += -2 * (data[i,-1]-data.iloc[i-2])\n",
        "    \n",
        "  bias_new = bias_new/lend(data)\n",
        "  for i in range(len(weights )):\n",
        "    weights[i]-= weights_new[i]/len(data)*learning_rate\n",
        "    \n",
        "  bias-=((bias_new)/len(data))*learning_rate\n",
        "    \n",
        "  return weights,bias\n",
        "    \n",
        "if __name__==\"__main__\":\n",
        "  data = get_dataset()\n",
        "  #print(data)\n",
        "  bias = random.randint(-1,1)\n",
        "  weights = np.random.normal(-1,1,len(data.columns)-1)\n",
        "  #print(weights, len(weights),len(data.columns))\n",
        "  mse =least_squares(data,weights,bias)\n",
        "  "
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n",
            "0   0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900  1.0  296.0   \n",
            "1   0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671  2.0  242.0   \n",
            "2   0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671  2.0  242.0   \n",
            "3   0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622  3.0  222.0   \n",
            "4   0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622  3.0  222.0   \n",
            "5   0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622  3.0  222.0   \n",
            "6   0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605  5.0  311.0   \n",
            "7   0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505  5.0  311.0   \n",
            "8   0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821  5.0  311.0   \n",
            "9   0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921  5.0  311.0   \n",
            "10  0.22489  12.5   7.87   0.0  0.524  6.377   94.3  6.3467  5.0  311.0   \n",
            "11  0.11747  12.5   7.87   0.0  0.524  6.009   82.9  6.2267  5.0  311.0   \n",
            "12  0.09378  12.5   7.87   0.0  0.524  5.889   39.0  5.4509  5.0  311.0   \n",
            "13  0.62976   0.0   8.14   0.0  0.538  5.949   61.8  4.7075  4.0  307.0   \n",
            "14  0.63796   0.0   8.14   0.0  0.538  6.096   84.5  4.4619  4.0  307.0   \n",
            "15  0.62739   0.0   8.14   0.0  0.538  5.834   56.5  4.4986  4.0  307.0   \n",
            "16  1.05393   0.0   8.14   0.0  0.538  5.935   29.3  4.4986  4.0  307.0   \n",
            "17  0.78420   0.0   8.14   0.0  0.538  5.990   81.7  4.2579  4.0  307.0   \n",
            "18  0.80271   0.0   8.14   0.0  0.538  5.456   36.6  3.7965  4.0  307.0   \n",
            "19  0.72580   0.0   8.14   0.0  0.538  5.727   69.5  3.7965  4.0  307.0   \n",
            "20  1.25179   0.0   8.14   0.0  0.538  5.570   98.1  3.7979  4.0  307.0   \n",
            "21  0.85204   0.0   8.14   0.0  0.538  5.965   89.2  4.0123  4.0  307.0   \n",
            "22  1.23247   0.0   8.14   0.0  0.538  6.142   91.7  3.9769  4.0  307.0   \n",
            "23  0.98843   0.0   8.14   0.0  0.538  5.813  100.0  4.0952  4.0  307.0   \n",
            "24  0.75026   0.0   8.14   0.0  0.538  5.924   94.1  4.3996  4.0  307.0   \n",
            "25  0.84054   0.0   8.14   0.0  0.538  5.599   85.7  4.4546  4.0  307.0   \n",
            "26  0.67191   0.0   8.14   0.0  0.538  5.813   90.3  4.6820  4.0  307.0   \n",
            "27  0.95577   0.0   8.14   0.0  0.538  6.047   88.8  4.4534  4.0  307.0   \n",
            "28  0.77299   0.0   8.14   0.0  0.538  6.495   94.4  4.4547  4.0  307.0   \n",
            "29  1.00245   0.0   8.14   0.0  0.538  6.674   87.3  4.2390  4.0  307.0   \n",
            "..      ...   ...    ...   ...    ...    ...    ...     ...  ...    ...   \n",
            "70  0.08826   0.0  10.81   0.0  0.413  6.417    6.6  5.2873  4.0  305.0   \n",
            "71  0.15876   0.0  10.81   0.0  0.413  5.961   17.5  5.2873  4.0  305.0   \n",
            "72  0.09164   0.0  10.81   0.0  0.413  6.065    7.8  5.2873  4.0  305.0   \n",
            "73  0.19539   0.0  10.81   0.0  0.413  6.245    6.2  5.2873  4.0  305.0   \n",
            "74  0.07896   0.0  12.83   0.0  0.437  6.273    6.0  4.2515  5.0  398.0   \n",
            "75  0.09512   0.0  12.83   0.0  0.437  6.286   45.0  4.5026  5.0  398.0   \n",
            "76  0.10153   0.0  12.83   0.0  0.437  6.279   74.5  4.0522  5.0  398.0   \n",
            "77  0.08707   0.0  12.83   0.0  0.437  6.140   45.8  4.0905  5.0  398.0   \n",
            "78  0.05646   0.0  12.83   0.0  0.437  6.232   53.7  5.0141  5.0  398.0   \n",
            "79  0.08387   0.0  12.83   0.0  0.437  5.874   36.6  4.5026  5.0  398.0   \n",
            "80  0.04113  25.0   4.86   0.0  0.426  6.727   33.5  5.4007  4.0  281.0   \n",
            "81  0.04462  25.0   4.86   0.0  0.426  6.619   70.4  5.4007  4.0  281.0   \n",
            "82  0.03659  25.0   4.86   0.0  0.426  6.302   32.2  5.4007  4.0  281.0   \n",
            "83  0.03551  25.0   4.86   0.0  0.426  6.167   46.7  5.4007  4.0  281.0   \n",
            "84  0.05059   0.0   4.49   0.0  0.449  6.389   48.0  4.7794  3.0  247.0   \n",
            "85  0.05735   0.0   4.49   0.0  0.449  6.630   56.1  4.4377  3.0  247.0   \n",
            "86  0.05188   0.0   4.49   0.0  0.449  6.015   45.1  4.4272  3.0  247.0   \n",
            "87  0.07151   0.0   4.49   0.0  0.449  6.121   56.8  3.7476  3.0  247.0   \n",
            "88  0.05660   0.0   3.41   0.0  0.489  7.007   86.3  3.4217  2.0  270.0   \n",
            "89  0.05302   0.0   3.41   0.0  0.489  7.079   63.1  3.4145  2.0  270.0   \n",
            "90  0.04684   0.0   3.41   0.0  0.489  6.417   66.1  3.0923  2.0  270.0   \n",
            "91  0.03932   0.0   3.41   0.0  0.489  6.405   73.9  3.0921  2.0  270.0   \n",
            "92  0.04203  28.0  15.04   0.0  0.464  6.442   53.6  3.6659  4.0  270.0   \n",
            "93  0.02875  28.0  15.04   0.0  0.464  6.211   28.9  3.6659  4.0  270.0   \n",
            "94  0.04294  28.0  15.04   0.0  0.464  6.249   77.3  3.6150  4.0  270.0   \n",
            "95  0.12204   0.0   2.89   0.0  0.445  6.625   57.8  3.4952  2.0  276.0   \n",
            "96  0.11504   0.0   2.89   0.0  0.445  6.163   69.6  3.4952  2.0  276.0   \n",
            "97  0.12083   0.0   2.89   0.0  0.445  8.069   76.0  3.4952  2.0  276.0   \n",
            "98  0.08187   0.0   2.89   0.0  0.445  7.820   36.9  3.4952  2.0  276.0   \n",
            "99  0.06860   0.0   2.89   0.0  0.445  7.416   62.5  3.4952  2.0  276.0   \n",
            "\n",
            "    PTRATIO       B  LSTAT     y  \n",
            "0      15.3  396.90   4.98  24.0  \n",
            "1      17.8  396.90   9.14  21.6  \n",
            "2      17.8  392.83   4.03  34.7  \n",
            "3      18.7  394.63   2.94  33.4  \n",
            "4      18.7  396.90   5.33  36.2  \n",
            "5      18.7  394.12   5.21  28.7  \n",
            "6      15.2  395.60  12.43  22.9  \n",
            "7      15.2  396.90  19.15  27.1  \n",
            "8      15.2  386.63  29.93  16.5  \n",
            "9      15.2  386.71  17.10  18.9  \n",
            "10     15.2  392.52  20.45  15.0  \n",
            "11     15.2  396.90  13.27  18.9  \n",
            "12     15.2  390.50  15.71  21.7  \n",
            "13     21.0  396.90   8.26  20.4  \n",
            "14     21.0  380.02  10.26  18.2  \n",
            "15     21.0  395.62   8.47  19.9  \n",
            "16     21.0  386.85   6.58  23.1  \n",
            "17     21.0  386.75  14.67  17.5  \n",
            "18     21.0  288.99  11.69  20.2  \n",
            "19     21.0  390.95  11.28  18.2  \n",
            "20     21.0  376.57  21.02  13.6  \n",
            "21     21.0  392.53  13.83  19.6  \n",
            "22     21.0  396.90  18.72  15.2  \n",
            "23     21.0  394.54  19.88  14.5  \n",
            "24     21.0  394.33  16.30  15.6  \n",
            "25     21.0  303.42  16.51  13.9  \n",
            "26     21.0  376.88  14.81  16.6  \n",
            "27     21.0  306.38  17.28  14.8  \n",
            "28     21.0  387.94  12.80  18.4  \n",
            "29     21.0  380.23  11.98  21.0  \n",
            "..      ...     ...    ...   ...  \n",
            "70     19.2  383.73   6.72  24.2  \n",
            "71     19.2  376.94   9.88  21.7  \n",
            "72     19.2  390.91   5.52  22.8  \n",
            "73     19.2  377.17   7.54  23.4  \n",
            "74     18.7  394.92   6.78  24.1  \n",
            "75     18.7  383.23   8.94  21.4  \n",
            "76     18.7  373.66  11.97  20.0  \n",
            "77     18.7  386.96  10.27  20.8  \n",
            "78     18.7  386.40  12.34  21.2  \n",
            "79     18.7  396.06   9.10  20.3  \n",
            "80     19.0  396.90   5.29  28.0  \n",
            "81     19.0  395.63   7.22  23.9  \n",
            "82     19.0  396.90   6.72  24.8  \n",
            "83     19.0  390.64   7.51  22.9  \n",
            "84     18.5  396.90   9.62  23.9  \n",
            "85     18.5  392.30   6.53  26.6  \n",
            "86     18.5  395.99  12.86  22.5  \n",
            "87     18.5  395.15   8.44  22.2  \n",
            "88     17.8  396.90   5.50  23.6  \n",
            "89     17.8  396.06   5.70  28.7  \n",
            "90     17.8  392.18   8.81  22.6  \n",
            "91     17.8  393.55   8.20  22.0  \n",
            "92     18.2  395.01   8.16  22.9  \n",
            "93     18.2  396.33   6.21  25.0  \n",
            "94     18.2  396.90  10.59  20.6  \n",
            "95     18.0  357.98   6.65  28.4  \n",
            "96     18.0  391.83  11.34  21.4  \n",
            "97     18.0  396.90   4.21  38.7  \n",
            "98     18.0  393.53   3.57  43.8  \n",
            "99     18.0  396.90   6.19  33.2  \n",
            "\n",
            "[100 rows x 14 columns]\n",
            "diff 1564.663548277282\n",
            "diff 1617.7719193178846\n",
            "diff 1575.1162565049747\n",
            "diff 1553.7214115705629\n",
            "diff 1584.134326359909\n",
            "diff 1576.5242751302023\n",
            "diff 1563.159737985666\n",
            "diff 1640.008409318605\n",
            "diff 1599.3799552947062\n",
            "diff 1571.416918954352\n",
            "diff 1607.9524912348586\n",
            "diff 1602.4231436981495\n",
            "diff 1479.2600778651265\n",
            "diff 1554.388671406351\n",
            "diff 1540.2006238026622\n",
            "diff 1536.5964060467375\n",
            "diff 1444.9874567738889\n",
            "diff 1557.8519272857802\n",
            "diff 1089.6203630429052\n",
            "diff 1545.831630146894\n",
            "diff 1552.4089008663943\n",
            "diff 1598.5147196413166\n",
            "diff 1616.1111124402614\n",
            "diff 1625.5352249474097\n",
            "diff 1612.9177287167727\n",
            "diff 1250.6982200985574\n",
            "diff 1540.2021016806189\n",
            "diff 1269.8019655094436\n",
            "diff 1592.6910707179127\n",
            "diff 1549.908475506719\n",
            "diff 1481.2954884913522\n",
            "diff 1559.419976316632\n",
            "diff 974.3763081009308\n",
            "diff 1478.0363298104353\n",
            "diff 1067.996585701332\n",
            "diff 1574.5418829581001\n",
            "diff 1487.386739716665\n",
            "diff 1516.5640145853217\n",
            "diff 1481.1905690288313\n",
            "diff 1520.460665574034\n",
            "diff 1510.9126920056967\n",
            "diff 1412.0562253834485\n",
            "diff 1411.551588940446\n",
            "diff 1452.2817633244827\n",
            "diff 1506.2627875305513\n",
            "diff 1517.362645589168\n",
            "diff 1516.6734069527497\n",
            "diff 1617.8136497791465\n",
            "diff 1653.2702308255164\n",
            "diff 1583.3060553383696\n",
            "diff 1546.5839896174834\n",
            "diff 1581.2200228012223\n",
            "diff 1501.0708028316187\n",
            "diff 1499.9919975162693\n",
            "diff 1482.066312862652\n",
            "diff 1550.1716987871052\n",
            "diff 1530.9189238901804\n",
            "diff 1565.8822658078207\n",
            "diff 1485.6656686832032\n",
            "diff 1544.8867659583293\n",
            "diff 1581.0311825166625\n",
            "diff 1575.8583304653312\n",
            "diff 1595.2467775394107\n",
            "diff 1538.3184836104756\n",
            "diff 1594.2838223709791\n",
            "diff 1474.5902982440373\n",
            "diff 1500.6182096575797\n",
            "diff 1448.5356950074538\n",
            "diff 1481.5268857434394\n",
            "diff 1476.5762297367492\n",
            "diff 1384.492045731396\n",
            "diff 1381.3126040525385\n",
            "diff 1412.9083429883815\n",
            "diff 1358.1656475010222\n",
            "diff 1384.302216690245\n",
            "diff 1427.2443574070642\n",
            "diff 1456.4785733041838\n",
            "diff 1441.7196152503584\n",
            "diff 1459.4790431593176\n",
            "diff 1455.119772692678\n",
            "diff 1516.3479089828058\n",
            "diff 1591.817372701273\n",
            "diff 1510.109113650575\n",
            "diff 1517.8413265483214\n",
            "diff 1546.0281231137872\n",
            "diff 1549.6020502335061\n",
            "diff 1533.8088640894262\n",
            "diff 1556.2827740135608\n",
            "diff 1618.6985103474285\n",
            "diff 1567.5147753608464\n",
            "diff 1553.016879278535\n",
            "diff 1575.4482571256337\n",
            "diff 1561.7201730416828\n",
            "diff 1512.3787290605783\n",
            "diff 1620.536075743881\n",
            "diff 1409.197570223148\n",
            "diff 1556.0743736301592\n",
            "diff 1607.3853534333198\n",
            "diff 1510.3742480216247\n",
            "diff 1570.8758808071495\n",
            "2293458.250904215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7LrtPpOjgDIH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-C76SIx-f_cO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}