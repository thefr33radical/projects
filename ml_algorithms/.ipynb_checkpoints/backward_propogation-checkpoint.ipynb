{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Backward Propogation\n",
    "\n",
    "Allows the information to go back from the cost backward through the network in order to compute the gradient. Therefore, loop over the nodes starting at the final node in reverse topological order to compute the derivative of the final node output with respect to each edgeâ€™s node tail. Doing so will help us know who is responsible for the most error and change the parameters in that direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backword(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    assert dA_prev.shape == A_prev.shape\n",
    "    assert dW.shape == W.shape\n",
    "    assert db.shape == b.shape\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation_fn):\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    dZ = relu_gradient(dA, activation_cache)\n",
    "    dA_prev, dW, db = linear_backword(dZ, linear_cache)\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def L_model_backward(AL, y, caches, hidden_layers_activation_fn=\"relu\"):\n",
    "    y = y.reshape(AL.shape)\n",
    "    L = len(caches)\n",
    "    grads = {}\n",
    "\n",
    "    dAL = np.divide(AL - y, np.multiply(AL, 1 - AL))\n",
    "\n",
    "    grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\n",
    "        \"db\" + str(L)] = linear_activation_backward(\n",
    "            dAL, caches[L - 1], \"sigmoid\")\n",
    "\n",
    "    for l in range(L - 1, 0, -1):\n",
    "        current_cache = caches[l - 1]\n",
    "        grads[\"dA\" + str(l - 1)], grads[\"dW\" + str(l)], grads[\n",
    "            \"db\" + str(l)] = linear_activation_backward(\n",
    "                grads[\"dA\" + str(l)], current_cache,\n",
    "                hidden_layers_activation_fn)\n",
    "\n",
    "    return grads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
